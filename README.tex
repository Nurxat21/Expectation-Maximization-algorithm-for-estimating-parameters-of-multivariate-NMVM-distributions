# Expectation-Maximization Algorithm for Estimating Parameters of Multivariate Non-Normal Variable Mixture Distributions

Obtaining the optimal solution for the aforementioned objective function becomes increasingly challenging as the dimensionality of the data exceeds three. The core principle of the Expectation-Maximization (EM) algorithm is to enhance the following augmented log-likelihood function:

\[
\bar{\delta} = \frac{1}{n} \sum_{i=1}^{n} \delta_i, \quad \bar{\eta} = \frac{1}{n} \sum_{i=1}^{n} \eta_i, \quad \bar{\xi} = \frac{1}{n} \sum_{i=1}^{n} \zeta_i
\]

In the context of generalized hyperbolic distributions, the variables are expressed as follows:

\[
\delta_i^{[k]} = \left(\frac{\mathcal{Q}^{[k]}_i + \chi^{[k]}}{\psi^{[k]} + \mathbf{\gamma}^{[k]} \Sigma^{[k]^{-1}} \mathbf{\gamma}^{[k]}}\right)^{-\frac{1}{2}} \frac{K_{\lambda - \frac{d}{2} - 1}\left(\sqrt{\left(\mathcal{Q}^{[k]}_i + \chi^{[k]}\right)\left(\psi^{[k]} + \mathbf{\gamma}^{[k]^{\top}} \Sigma^{[k]-1} \mathbf{\gamma}^{[k]}\right)}\right)}{K_{\lambda - \frac{d}{2}}\left(\sqrt{\left(\mathcal{Q}^{[k]}_i + \chi^{[k]}\right)\left(\psi^{[k]} + \mathbf{\gamma}^{[k]^{\top}} \Sigma^{[k]^{-1}} \mathbf{\gamma}^{[k]}\right)}\right)}
\]

\[
\eta_i^{[k]} = \left(\frac{\mathcal{Q}^{[k]}_i + \chi^{[k]}}{\psi^{[k]} + \mathbf{\gamma}^{[k]^{\top}} \Sigma^{[k]^{-1}} \mathbf{\gamma}^{[k]}}\right)^{\frac{1}{2}} \frac{K_{\lambda - \frac{d}{2} + 1}\left(\sqrt{\left(\mathcal{Q}^{[k]}_i + \chi^{[k]}\right)\left(\psi^{[k]} + \mathbf{\gamma}^{[k]^{\top}} \Sigma^{[k]^{-1}} \mathbf{\gamma}^{[k]}\right)}\right)}{K_{\lambda - \frac{d}{2}}\left(\sqrt{\left(\mathcal{Q}^{[k]}_i + \chi^{[k]}\right)\left(\psi^{[k]} + \mathbf{\gamma}^{[k]^{\top}} \Sigma^{[k]^{-1}} \mathbf{\gamma}^{[k]}\right)}\right)}
\]

\[
\begin{aligned}
\zeta_i^{[k]} & = \frac{1}{2} \log \left(\frac{\mathcal{Q}^{[k]}_i + \chi^{[k]}}{\psi^{[k]} + \mathbf{\gamma}^{[k]^{\top}} \Sigma^{[k]^{-1}} \mathbf{\gamma}^{[k]}\right)
\end{aligned}
\]

This formulation serves to leverage the properties of generalized hyperbolic distributions to facilitate parameter estimation within the scope of the EM algorithm.{-1}} \mathbf{\gamma}^{[k]}}\right)+ \\
& \frac{\left.\frac{\partial K_{\lambda-\frac{d}{2}+\alpha}\left(\sqrt{\left(\mathcal{Q}^{[k]}_i+\chi^{[k]}\right)\left(\psi^{[k]}+\mathbf{\gamma}^{[k]^{\top}} \Sigma^{[k]^{-1}} \mathbf{\gamma}^{[k]}\right)}\right)}{\partial \alpha}\right|_{\alpha=0}}{K_{\lambda-\frac{d}{2}}\left(\sqrt{\left(\mathcal{Q}^{[k]}_i+\chi^{[k]}\right)\left(\psi^{[k]}+\mathbf{\gamma}^{[k]^{\top}} \Sigma^{[k]^{-1}} \mathbf{\gamma}^{[k]}\right)}\right)} .
\end{aligned}
\end{equation}

For the variance gamma distribution, we just need to set $\chi = 0$ in above equations, in which we have 
\begin{equation}
    W_i \mid \mathbf{X_i} \sim N^- (\lambda - \frac{d}{2}, \mathcal{Q}(x_i), \psi + \mathbf{\gamma^{\top} \Sigma^{-1} \gamma})
\end{equation}
For the multivariate Skew-t distribution, we have 
\begin{equation}
    W_i \mid \mathbf{X_i} \sim N^- (-\frac{d + \nu}{2}, \mathcal{Q}(x_i) + \nu, \mathbf{\gamma^{\top} \Sigma^{-1} \gamma})
\end{equation}
where $\delta^{[k]}$, $\eta^{[k]}$ and $\zeta_i^{[k]}$ show as 
\begin{equation}
    \delta_i^{[k]}=\left(\frac{\mathcal{Q}^{[k]}_i+\nu^{[k]}}{\mathbf{\gamma}^{[k]^2} \Sigma^{[k]-1} \mathbf{\gamma}^{[k]}}\right)^{-\frac{1}{2}} \frac{K_{\frac{\nu+d+2}{2}}\left(\sqrt{\left(\mathcal{Q}^{[k]}_i+\nu^{[k]}\right)\left(\mathbf{\gamma}^{[k]^{\top}} \Sigma^{[k]^{-1}} \mathbf{\gamma}^{[k]}\right)}\right)}{K_{\frac{\nu+d}{2}}\left(\sqrt{\left(\mathcal{Q}^{[k]}_i+\nu^{[k]}\right)\left(\mathbf{\gamma}^{[k]^{\top} \Sigma^{[k]]^{-1}} \mathbf{\gamma}^{[k]}}\right)}\right)}
\end{equation}

\begin{equation}
    \eta_i^{[k]}=\left(\frac{\mathcal{Q}^{[k]}+\nu^{[k]}}{\mathbf{\gamma}^{[k]^{\top}} \Sigma^{[k]^{-1}} \mathbf{\gamma}^{[k]}}\right)^{\frac{1}{2}} \frac{K_{\frac{\nu+d-2}{2}}\left(\sqrt{\left(\mathcal{Q}^{[k]}+\nu^{[k]}\right)\left(\mathbf{\gamma}^{[k]^{\top}} \Sigma^{[k]^{-1}} \mathbf{\gamma}^{[k]}\right)}\right)}{K_{\frac{\nu+d}{2}}\left(\sqrt{\left(\mathcal{Q}^{[k]}+\nu^{[k]}\right)\left(\mathbf{\gamma}^{\left.[k]^{\top} \Sigma^{[k]}\right]^{-1}} \mathbf{\gamma}^{[k]}\right)}\right)}
\end{equation}

\begin{equation}
    \begin{aligned}
\zeta_i^{[k]} & =\frac{1}{2} \log \left(\frac{\mathcal{Q}^{[k]}+\nu^{[k]}}{\mathbf{\gamma}^{[k]^{\top}} \Sigma^{[k]^{-1}} \mathbf{\gamma}^{[k]}}\right)+ \\
& \frac{\left.\frac{\partial K_{-\frac{\nu+d}{2}+\alpha}\left(\sqrt{\left(\mathcal{Q}^{[k]}+\nu^{[k]}\right)\left(\mathbf{\gamma}^{[k]^{\top}} \Sigma^{[k]^{-1}} \mathbf{\gamma}^{[k]}\right)}\right)}{\partial \alpha}\right|_{\alpha=0}}{K_{\frac{v_{+d}}{2}}\left(\sqrt{\left(\mathcal{Q}^{[k]}+\nu^{[k]}\right)\left(\mathbf{\gamma}^{[k]^{\top}} \Sigma^{[k]-1} \mathbf{\gamma}^{[k]}\right)}\right)} .
\end{aligned}
\end{equation}

In the M-step, we need to replace the latent variables $\omega^{-1}_i$ by $\delta^{[k]}_i$, $\omega_i$ by $\eta^{[k]}_i$, $log(\omega_i)$ by $\zeta^{[k]}_i$ in the maximization. Thus, maximizing the conditional expectation of $L_1$, we can get the k-step estimations of $\mathbf{\gamma}^{[k+1]}$, $\mathbf{\mu}^{[k+1]}$ and $\mathbf{\Sigma}^{[k+1]}$. During the maximizing of conditional expectation of $L_2$, we need to solve (\ref{Solving_Equation_alpha}) to obtain the $\alpha^{[k+1]}$. When we get the $\alpha^{[k+1]}$, we can update the values of $\chi^{[k+1]}$ and $\psi^{[k+1]}$. 


When we calibrated the parameter of GH distribution, we faced an identification problem. In \cite{Mcneil_A_J_And_Frey_R_And_Embrechts_P_2015}, they used a fixed number $c$ to be the determinant of $\Sigma$, in which the number $c$ is the determinate of the sample covariance matrix to solve such problem by using (\ref{Sigma_GH_estimation}) and set
\begin{equation}\label{Sigma_K_1}
    \Sigma^{[k+1]}:=\frac{c^{1 / d} \Sigma^{[k+1]}}{\left|\Sigma^{[k+1]}\right|^{1 / d}}
\end{equation}

As mentioned in \cite{Hu_Wenbo_2005}, when $|\lambda|$ is large, the (\ref{Solving_Equation_alpha}) may not be equalled to zero so that we will minimize the square root of (\ref{Solving_Equation_alpha}) to update the $\alpha^{[k+1]}$. In \cite{Hu_Wenbo_2005}, he set $\chi$ or $\psi$ to be constant, when $|\lambda|$ is large. However, different choices of constant $\chi$ or $\psi$ would lead to different estimating speeds, and it might crash in some special constant values. 

The EM algorithm iteratively updates the parameters to maximize the likelihood function. The iterative process begins with initial estimates for the parameters \(\xi = (\lambda, \chi, \psi, \Sigma, \mu, \gamma)\). In the Expectation Step (E-step), the expected value of the complete-data log-likelihood function is calculated with respect to the current parameter estimates, involving the computation of the expected values of the latent mixing variables \(w_i\) given the observed data. The Maximization Step (M-step) follows, where the expected complete-data log-likelihood function obtained in the E-step is maximized to update the parameter estimates, including the location parameter \(\mu\), the dispersion parameter \(\Sigma\) using the optimization (\ref{Sigma_K_1})to ensure positive definiteness, the skewness parameter \(\gamma\), and the parameters of the generalized inverse Gaussian (GIG) distribution \(\lambda\), \(\chi\), and \(\psi\). 
